## Lecture11 渐进分析（Asymptotics）



### **第一步：为什么要学习渐进分析？ (An Introduction to Asymptotic Analysis)**

在我们编程时，除了关心代码能否正确运行，还必须关心它的**执行成本（Execution Cost）**，主要包括两个方面：

1. **时间复杂度（Time complexity）**：程序运行需要多长时间？
2. **空间复杂度（Space complexity）**：程序需要占用多少内存？

我们今天的重点是**时间复杂度**。

想象一个问题：**在一个已排序的数组中，检查是否存在重复元素。**

例如数组：`[-3, -1, 2, 4, 4, 8]`

**思考一下，你有几种方法来解决这个问题？**

- **算法一（朴素算法）**： 我们可以用两层循环，比较数组中每一对元素。第一个元素和后面所有元素比，第二个元素和后面所有元素比...

  ```java
  // 算法一：dup1 (duplicate 1)
  public static boolean dup1(int[] A) {
      for (int i = 0; i < A.length; i += 1) {
          for (int j = i + 1; j < A.length; j += 1) {
              if (A[i] == A[j]) {
                  return true;
              }
          }
      }
      return false;
  }
  ```

- **算法二（利用有序性）**： 因为数组是排好序的，所以如果存在重复元素，它们一定是相邻的。我们只需要遍历一次，比较每个元素和它旁边的元素是否相等即可。

  ```java
  // 算法二：dup2 (duplicate 2)
  public static boolean dup2(int[] A) {
      for (int i = 0; i < A.length - 1; i += 1) {
          if (A[i] == A[i+1]) {
              return true;
          }
      }
      return false;
  }
  ```

直觉上，`dup2` 比 `dup1` 快得多，因为它做的事情更少。但**“快多少”**呢？我们如何精确、科学地量化它们的效率差异？这就是渐进分析要解决的核心问题。

------



### **第二步：如何衡量代码的运行时间？(Runtime Characterization)**

为了量化效率，我们有两种基本方法：

**方法1：物理计时** 用秒表或计算机的计时工具，直接测量程序运行花费的时间。

- **优点**：简单、直观。
- **缺点**：
  - 结果依赖于计算机的硬件性能。在我的电脑上跑 5 秒，在你的超级计算机上可能只要 0.5 秒。
  - 结果依赖于特定的输入。对于某些输入，算法可能很快就结束了。
  - 不够精确，难以形成严谨的数学理论。

**方法2：计算操作数** 与其计时，不如我们来数一数代码执行了多少次**基本操作**（如赋值、比较、数组访问等）。这种方法更科学，因为它与硬件无关。

我们以 `dup1` 为例，当输入数组大小为 `N` 时，来做一个最坏情况（即没有重复元素）的分析：

| 操作           | 符号计数 (Symbolic Count) |
| -------------- | ------------------------- |
| `i = 0`        | 1                         |
| `i < A.length` | N + 1                     |
| `i += 1`       | N                         |
| `j = i + 1`    | N                         |
| `j < A.length` | ≈ N²/2                    |
| `j += 1`       | ≈ N²/2                    |
| `A[i] == A[j]` | ≈ N²/2                    |

**挑战**：精确计算每一条语句的执行次数非常繁琐！而且不同操作的耗时也略有不同。我们似乎把问题搞得更复杂了。有没有更简单的方法？

------



### **第三步：化繁为简，抓住核心 (Asymptotic Behavior)**

在分析算法时，我们真正关心的是当**输入规模 N 变得非常非常大**时，运行时间的**增长趋势**。这就是**渐进行为（Asymptotic Behavior）**。

例如，对于 `dup1`，当 `N` 变得巨大时，总操作数大约是 `(N²/2) + (N²/2) + ...`。其中 `N²` 这一项是起决定性作用的，其他的 `N` 或者常数项，在 `N²` 面前都无足轻重。

为了抓住这个核心，我们可以对分析过程进行四项简化：

1. **只考虑最坏情况 (Worst Case)**：我们通常对算法性能的“下限”最感兴趣，即在最糟糕的输入下它的表现如何。
2. **选择一个代表性操作 (Cost Model)**：挑选代码中执行最频繁、最能代表算法核心逻辑的操作作为我们的**成本模型**。例如，在 `dup1` 中，我们可以选择 `A[i] == A[j]` 这个比较操作作为代表。
3. **忽略低阶项 (Eliminate Low Order Terms)**：在描述增长趋势时，我们只保留最高阶的项。例如，如果精确计算出操作数是 `3N² + 100N + 5`，我们只保留 `3N²`，忽略 `100N` 和 `5`。
4. **忽略常数系数 (Eliminate Multiplicative Constants)**：我们不关心最高阶项的系数是 3、1/2 还是 100，只关心它的“形状”。`3N²`、`100N²` 都被我们看作是 `N²` 级别的。

**应用简化规则：** 让我们用这个简化流程重新分析 `dup1` 和 `dup2`。

- **分析 `dup1` (朴素算法):**

  1. **最坏情况**: 数组中没有重复项。
  2. **成本模型**: 选择 `A[i] == A[j]` 的比较次数。
  3. **计数**: `j` 循环的次数依赖于 `i`。
     - 当 `i = 0`, `j` 循环 `N-1` 次。
     - 当 `i = 1`, `j` 循环 `N-2` 次。
     - ...
     - 当 `i = N-2`, `j` 循环 `1` 次。 总比较次数为：`(N-1) + (N-2) + ... + 1`，这是一个等差数列求和，结果是 `N(N-1)/2 = N²/2 - N/2`。
  4. **简化**:
     - 忽略低阶项 `-N/2`，剩下 `N²/2`。
     - 忽略常数系数 `1/2`，剩下 `N²`。

  - **结论**: `dup1` 算法的运行时间**增长级别**是 `N²`。

- **分析 `dup2` (优化算法):**

  1. **最坏情况**: 数组中没有重复项。
  2. **成本模型**: 选择 `A[i] == A[i+1]` 的比较次数。
  3. **计数**: 循环从 `i=0` 到 `N-2`，总共执行 `N-1` 次比较。
  4. **简化**:
     - `N-1` 的最高阶项是 `N`。
     - 忽略常数系数 `1`，剩下 `N`。

  - **结论**: `dup2` 算法的运行时间**增长级别**是 `N`。

现在，我们得到了一个非常清晰和强大的结论：`dup1` 的复杂度是 `N²` 级别，而 `dup2` 是 `N` 级别。这意味着当输入规模 `N` 增大时，`dup1` 的耗时会急剧增长，而 `dup2` 的耗时只是线性增加。

------



### **第四步：引入数学符号，精炼表达 (Big Theta & Big O)**

为了用数学语言精确地描述我们上面得出的“增长级别”，我们引入了渐进记号。



#### **1. Big Theta (Θ) - 紧确界**

Big Theta (Θ) 是我们最常用的记号，它精确地描述了函数的增长级别。

- **口头定义**: “运行时间 R(N) 的增长级别是 f(N)”。
- **严谨定义**: 如果我们说 `R(N) ∈ Θ(f(N))`，意味着存在正常数 `k₁` 和 `k₂`，以及一个阈值 `N₀`，当 `N > N₀` 时，满足 `k₁ * f(N) ≤ R(N) ≤ k₂ * f(N)`。



**Big Theta (Θ) — 三明治法则 (The Sandwich Rule)**

> **核心思想：** 两个函数的增长速度**完全一样**。

**定义：** 我们说 `R(N)` 是 `Θ(f(N))`，意思是：当 `N` 足够大时，你能用 `f(N)` 的两个版本（一个放大一点，一个缩小一点）把 `R(N)` **像三明治一样夹在中间**。

- **上层面包**: `k₂ * f(N)` (放大版的 `f(N)`)
- **中间馅料**: `R(N)` (你算法的实际运行时间)
- **下层面包**: `k₁ * f(N)` (缩小版的 `f(N)`)

**直观例子：** 假设你的算法运行时间是 `R(N) = 2N + 10`。 我们要比较的参考函数是 `f(N) = N`。

- `R(N)` 会不会比 `1 * N` 增长得慢？不会，它永远在 `1*N` 上面。 (这是**下层面包**)
- `R(N)` 会不会比 `3 * N` 增长得快？也不会，当 `N` 足够大时（`N > 5`），`3*N` 就会永远压在 `2N+10` 上面。(这是**上层面包**)

因为我们成功地用 `1*N` 和 `3*N` 把 `2N + 10` 夹在了中间，所以我们可以说，`2N + 10` 的复杂度**就是** `Θ(N)`。

**一句话记住 Θ：** 你的算法被参考函数 `f(N)` **“卡死”**了，它只能和 `f(N)` 一起成长。



#### **2. Big O (O) - 上界**

Big O (O) 提供了一个算法运行时间的**上限**。它表示“算法的增长速度**不会快于**这个级别”。

- **口头定义**: “运行时间 R(N) 的增长级别最多是 f(N)”。
- **严谨定义**: 如果我们说 `R(N) ∈ O(f(N))`，意味着存在正常数 `k` 和一个阈值 `N₀`，当 `N > N₀` 时，满足 `R(N) ≤ k * f(N)`。



##### **Big O (O) — 天花板法则 (The Ceiling Rule)**

> **核心思想：** 一个函数的增长速度**不会超过**另一个。

**定义：** 我们说 `R(N)` 是 `O(f(N))`，意思是：当 `N` 足够大时，你总能找到一个放大版的 `f(N)`，把它当做**天花板**，而 `R(N)` 永远无法突破这个天花板。

- **天花板**: `k * f(N)` (某个放大版的 `f(N)`)
- **你的位置**: `R(N)` (你算法的实际运行时间，永远在天花板下面)

**直观例子：** 假设你的算法运行时间是 `R(N) = 2N + 10`。 我们要比较的参考函数是 `f(N) = N²`。

- 我们能找到一个 `k`，让 `k * N²` 成为 `2N+10` 的天花板吗？
- 当然可以！哪怕 `k=1`，当 `N` 稍微大一点（`N > 5`），`N²` 的增长速度会远远甩开 `2N+10`，`2N+10` 将永远无法追上 `N²`。
- 所以，我们可以说 `2N + 10` 的复杂度是 `O(N²)`。这个说法是**正确**的，它保证了算法的性能**不会比 `N²` 更差**。

当然，用 `f(N) = N` 来做天花板也可以（比如用 `3*N`），所以 `2N+10` 也是 `O(N)`。在实际应用中，我们总是选择**最低的那个天花板**，即 `O(N)`。

**一句话记住 O：** 你的算法被参考函数 `f(N)` **“罩住”了，它的表现不会比 `f(N)` 更差**。



**为什么需要 Big O?** 在很多情况下，精确分析出 Θ 很难，或者一个算法在不同情况下的表现差异巨大。这时，给出一个最坏情况下的保证（即 Big O 上界）就非常有用了。在求职面试和日常交流中，人们经常使用 Big O 来泛指算法的时间复杂度，尽管他们心里想的往往是 Big Theta。





### **第五分：分析循环 (For Loops)**

分析循环是时间复杂度分析的基础，关键在于**识别循环的执行次数**。



#### **情况1：简单循环**

这是最简单的情况，循环次数直接由输入 `N` 决定。

```java
// 打印 0 到 N-1
public void bar(int N) {
    for (int i = 0; i < N; i += 1) {
        System.out.println("hello"); // 这个操作是 Θ(1)
    }
}
```

- **分析**：循环体内的 `println` 是一个常数时间操作，我们记为 `Θ(1)`。这个循环会执行 `N` 次。因此，总时间是 `N * Θ(1)`。
- **结论**：`bar` 方法的时间复杂度是 `Θ(N)`。



#### **情况2：乘法/除法变化的循环**

如果循环变量不是 `+1`，而是 `*2` 呢？

```java
public void baz(int N) {
    for (int i = 1; i < N; i *= 2) {
        System.out.println("hello");
    }
}
```

- **分析**：我们来看 `i` 的变化序列：`1, 2, 4, 8, 16, ...` 直到它大于或等于 `N`。 假设循环执行了 `k` 次，那么第 `k` 次时 `i` 的值是 `2^(k-1)`。循环结束的条件是 `2^(k-1) >= N`。 对这个不等式两边取对数，我们得到 `k-1 >= log₂(N)`，所以 `k` 大约是 `log₂(N)`。
- **结论**：循环执行了 `log₂(N)` 次，所以 `baz` 方法的复杂度是 `Θ(log N)`。

**核心思想：**

- 如果循环变量是 `+` 或 `-`，复杂度通常是**线性**的。
- 如果循环变量是 `*` 或 `/`，复杂度通常是**对数**的。



#### **情况3：嵌套循环 (Nested Loops)**

这是最常见也是最容易出错的地方。分析的关键是**外层循环每执行一次，内层循环执行多少次**。

**a) 简单嵌套**

```java
public void qux(int N) {
    for (int i = 0; i < N; i += 1) {       // 外层执行 N 次
        for (int j = 0; j < N; j += 1) {   // 内层执行 N 次
            System.out.println("hello");   // Θ(1) 操作
        }
    }
}
```

- **分析**：外层循环执行 `N` 次。对于外层的**每一次**循环，内层循环都要完整地执行 `N` 次。总执行次数是 `N * N = N²`。
- **结论**：`qux` 方法的复杂度是 `Θ(N²)`。

**b) 依赖性嵌套** 内层循环的次数依赖于外层循环的变量。

```java
public void quux(int N) {
    for (int i = 0; i < N; i += 1) {         // 外层执行 N 次
        for (int j = 0; j < i; j += 1) {     // 内层执行 i 次
            System.out.println("hello");
        }
    }
}
```

- **分析**：
  - 当 `i = 0` 时，内层循环执行 0 次。
  - 当 `i = 1` 时，内层循环执行 1 次。
  - 当 `i = 2` 时，内层循环执行 2 次。
  - ...
  - 当 `i = N-1` 时，内层循环执行 `N-1` 次。 总执行次数是 `0 + 1 + 2 + ... + (N-1)`。这是一个等差数列求和，结果是 `(N-1) * N / 2 = N²/2 - N/2`。
- **结论**：根据我们上一节课学到的简化法则，我们忽略低阶项 `-N/2` 和系数 `1/2`，得到 `quux` 方法的复杂度是 `Θ(N²)`。

------



### **第六步：分析递归 (Recursion)**

分析递归比分析循环要更抽象一些。我们无法直接数循环次数，而是需要建立一个**递推关系式 (Recurrence Relation)** 来描述其运行时间。

分析步骤：

1. **建立递推式 `T(N)`**：`T(N)` 表示输入规模为 `N` 时的运行时间。它通常等于**非递归部分的工作量**加上**所有递归调用的工作量**。
2. **求解递推式**：通过展开或使用公式来求解 `T(N)`。



#### **例子1：二分查找 (Binary Search)**

二分查找是在一个**有序数组**中查找元素的经典算法。

```java
// 在有序数组 a 中查找 x
public boolean binarySearch(int[] a, int x) {
    return binarySearchHelper(a, x, 0, a.length - 1);
}

// 辅助函数，在 a[L..R] 范围内查找 x
private boolean binarySearchHelper(int[] a, int x, int L, int R) {
    if (L > R) return false;                  // Base case: Θ(1)
    int M = (L + R) / 2;                      // 中点: Θ(1)
    if (a[M] == x) return true;               // 找到: Θ(1)
    else if (a[M] < x) {
        return binarySearchHelper(a, x, M + 1, R); // 递归调用
    } else {
        return binarySearchHelper(a, x, L, M - 1); // 递归调用
    }
}
```

- **分析**：
  1. **建立递推式**：
     - 输入规模是数组的长度 `N = R - L + 1`。
     - 在 `binarySearchHelper` 函数中，非递归部分的工作是几次比较和计算，这是 `Θ(1)` 的。
     - 之后，它**只对一半的数组**进行了一次递归调用，这部分的工作量是 `T(N/2)`。
     - 所以，递推式是：`T(N) = Θ(1) + T(N/2)`。
  2. **求解递推式**：
     - `T(N) = 1 + T(N/2)`
     - `= 1 + (1 + T(N/4))`
     - `= 1 + 1 + (1 + T(N/8))`
     - ...
     - 这个过程持续了多少次？也就是，`N` 可以被连续除以 2 多少次，直到它变成 1？答案是 `log₂(N)` 次。
- **结论**：二分查找的时间复杂度是 `Θ(log N)`。



#### **例子2：归并排序 (Merge Sort)**

归并排序是一种高效的排序算法，它完美地体现了递归的力量。

```java
public void mergeSort(int[] arr, int L, int R) {
    if (L >= R) return;                  // Base case
    int M = (L + R) / 2;
    mergeSort(arr, L, M);                // 递归调用 1: T(N/2)
    mergeSort(arr, M + 1, R);            // 递归调用 2: T(N/2)
    merge(arr, L, M, R);                 // 合并两个有序子数组
}
```

`merge` 函数的作用是将两个已经排好序的子数组 `arr[L..M]` 和 `arr[M+1..R]` 合并成一个大的有序数组。这个合并操作需要遍历两个子数组的所有元素，因此 `merge` 的时间复杂度是 `Θ(N)`，其中 `N = R - L + 1`。

- **分析**：
  1. **建立递推式**：
     - 输入规模是 `N`。
     - `mergeSort` 函数进行了**两次**对规模为 `N/2` 的子问题的递归调用，工作量是 `2 * T(N/2)`。
     - 它还进行了一次 `merge` 操作，工作量是 `Θ(N)`。
     - 所以，递推式是：`T(N) = 2T(N/2) + Θ(N)`。
  2. **求解递推式 (递归树法)**： 我们可以把这个过程画成一棵树。
     - **第 0 层 (根)**：`merge` 一整个 `N` 规模的数组，工作量 `N`。
     - **第 1 层**：`merge` 两个 `N/2` 规模的数组，总工作量 `2 * (N/2) = N`。
     - **第 2 层**：`merge` 四个 `N/4` 规模的数组，总工作量 `4 * (N/4) = N`。
     - ...
     - **树的高度**：这棵树有多少层？和二分查找一样，是 `log₂(N)` 层。
     - **总工作量**：每一层的工作量都是 `N`，总共有 `log N` 层。所以总工作量是 `N * log N`。
- **结论**：归并排序的时间复杂度是 `Θ(N log N)`。





### **总结与练习**

恭喜你！我们已经走完了渐进分析的核心学习路径。回顾一下：

1. 我们从一个实际问题出发，认识到**量化算法效率**的重要性。
2. 我们尝试了**物理计时**和**操作数统计**，并发现精确统计过于繁琐。
3. 我们学会了**化繁为简**，通过**四项简化规则**（抓最坏情况、定成本模型、去低阶项、去系数）来找到运行时间的**增长级别**。
4. 我们引入了**Θ (Big Theta)** 和 **O (Big O)** 这两个数学记号，来严谨地描述算法的紧确界和上界。
   - **Θ (Big Theta)**：紧密边界，相当于“=”。
   - **O (Big O)**：上界，相当于“≤”。



#### 常见复杂度

| 复杂度       | 名称         | 例子                  | 效率评估 (当 N=1,000,000)             |
| ------------ | ------------ | --------------------- | ------------------------------------- |
| `Θ(1)`       | **常数**     | 访问数组元素 `arr[i]` | 极快 (瞬间完成)                       |
| `Θ(log N)`   | **对数**     | 二分查找              | 非常快 (约 20 次操作)                 |
| `Θ(N)`       | **线性**     | 遍历数组              | 很快 (百万次操作，现代计算机可接受)   |
| `Θ(N log N)` | **线性对数** | 归并排序              | 较快 (约 2千万次操作，高效排序的标志) |
| `Θ(N²)`      | **平方**     | 简单嵌套循环          | 慢 (万亿次操作，对于大 N 无法接受)    |
| `Θ(2^N)`     | **指数**     | 朴素的斐波那契递归    | 极慢 (宇宙毁灭也算不完)               |

